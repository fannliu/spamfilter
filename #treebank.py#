# Natural Language Toolkit: Tokenizers
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Edward Loper <edloper@gradient.cis.upenn.edu>
# URL: <http://nltk.sourceforge.net>
# For license information, see LICENSE.TXT

"""
A regular-expression based word tokenizer that tokenizes sentences
using the conventions used by the Penn Treebank.
"""

import re

######################################################################
#{ Regexp-based treebank tokenizer
######################################################################
# (n.b., this isn't derived from RegexpTokenizer)

class TreebankWordTokenizer():
    """
    A word tokenizer that tokenizes sentences using the conventions
    used by the Penn Treebank.  Contractions, such as "can't", are
    split in to two tokens.  E.g.:

      - can't S{->} ca n't
      - he'll S{->} he 'll
      - weren't S{-} were n't

    This tokenizer assumes that the text has already been segmented into
    sentences.  Any periods -- apart from those at the end of a string --
    are assumed to be part of the word they are attached to (e.g. for
    abbreviations, etc), and are not separately tokenized.
    """
    # List of contractions adapted from Robert MacIntyre's tokenizer.
    CONTRACTIONS2 = [re.compile(r"(?i)(.)('ll|'re|'ve|n't|'s|'m|'d)\b"),
                     re.compile(r"(?i)\b(can)(not)\b"),
                     re.compile(r"(?i)\b(D)('ye)\b"),
                     re.compile(r"(?i)\b(Gim)(me)\b"),
                     re.compile(r"(?i)\b(Gon)(na)\b"),
                     re.compile(r"(?i)\b(Got)(ta)\b"),
                     re.compile(r"(?i)\b(Lem)(me)\b"),
                     re.compile(r"(?i)\b(Mor)('n)\b"),
                     re.compile(r"(?i)\b(T)(is)\b"),
                     re.compile(r"(?i)\b(T)(was)\b"),
                     re.compile(r"(?i)\b(Wan)(na)\b")]
    CONTRACTIONS3 = [re.compile(r"(?i)\b(Whad)(dd)(ya)\b"),
                     re.compile(r"(?i)\b(Wha)(t)(cha)\b")]

    def tokenize(self, text):
        for regexp in self.CONTRACTIONS2:
            text = regexp.sub(r'\1 \2', text)
        for regexp in self.CONTRACTIONS3:
            text = regexp.sub(r'\1 \2 \3', text)

        # Separate most punctuation
        text = re.sub(r"([^\w\.\'\-\/,&])", r' \1 ', text)

        # Separate commas if they're followed by space.
        # (E.g., don't separate 2,500)
        text = re.sub(r"(,\s)", r' \1', text)

        # Separate single quotes if they're followed by a space.
        text = re.sub(r"('\s)", r' \1', text)

        # Separate periods that come before newline or end of string.
        text = re.sub('\. *(\n|$)', ' . ', text)

        return text.split()

    def tokenize_by_web(self, text):
        new_text = self.tokenize(text)
        # find all tokenized html tags
        # this is <word> should have no other words in there
        # </word> should work too
        ret_text = []
        count = 0
        for i in range(len(new_text)):
            if i+2 < len(new_text):
                if new_text[i] == '<' and new_text[i+2] == '>':
                    # then add this as an html tag (potentially make specific tags)
                    count += 1
                    ret_text.append(new_text[i] + new_text[i+1] + new_text[i+2])
                elif count != 0 and count < 2:
                    count += 1
                elif count == 2:
                    count = 0
                else:
                    ret_text.append(new_text[i])
        return ret_text

    def tokenize_by_web_with_overpunc(self, text):
        ret_text = self.tokenize_by_web(text)
        # overpunctuation tokenization (don't split up ===)
        curr_exclaimed = []
        has_exclaimed = False
        punc = ['=']
        next_text = []
        for i in range(len(ret_text)):
            for p in punc:
                if ret_text[i] == p and not has_exclaimed:
                    has_exclaimed = True
                    curr_exclaimed.append(p)
                elif ret_text[i] == p and has_exclaimed:
                    curr_exclaimed.append(p)
                elif ret_text[i] != p and has_exclaimed:
                    has_exclaimed = False
                    new_exclaim = ''.join(curr_exclaimed)
                    next_text.append(new_exclaim)
                else:
                    next_text.append(ret_text[i])
        return next_text

    def tokenize_money(self, text):
        # moneytag = re.compile('($)|(%)|(\d+.\d{2})|(\d+)')
        # try just leaving in digits with their % or $, then
        # count how many tokens are digits with % and $
        digits = re.compile('\.*\d+')
        ret_text = []
        count = 0
        new_text = self.tokenize_by_web_with_overpunc(text)
        num_money_tokens = 0
        for i in range(len(new_text)):
            if i+2 < len(new_text):
                if (new_text[i] == '$' and digits.match(new_text[i+1])) \
                        or (digits.match(new_text[i]) and new_text[i+1] == '%') \
                        or ((new_text[i].lower() == 'click' or \
                                 new_text[i].lower() == 'order') and \
                            (new_text[i+1].lower() == 'here' or \
                                 new_text[i+1].lower() == 'me' or \
                                 new_text[i+1].lower() == 'this' or \
                                 new_text[i+1].lower() == 'now')):
                    # then add this as a whole either $1.00, or 40% etc.
                    count += 1
                    ret_text.append(new_text[i] + new_text[i+1])
                    num_money_tokens += 1
                elif count == 1:
                    count = 0
                else:
                    ret_text.append(new_text[i])
        return (ret_text, num_money_tokens)

    def get_money_tokens(self, text):
        # moneytag = re.compile('($)|(%)|(\d+.\d{2})|(\d+)')
        # try just leaving in digits with their % or $, then
        # count how many tokens are digits with % and $
        digits = re.compile('\.*\d+')
        money_tokens = []
        count = 0
        new_text = self.tokenize_by_web_with_overpunc(text)
        for i in range(len(new_text)):
            if i+2 < len(new_text):
                if (new_text[i] == '$' and digits.match(new_text[i+1])) or (digits.match(new_text[i]) and new_text[i+1] == '%'):
                    # then add this as a whole either $1.00, or 40% etc.
                    count += 1
                    money_tokens.append(new_text[i] + new_text[i+1])
                    # print new_text[i] + new_text[i+1]
                elif count == 1:
                    count = 0
        return money_tokens

    def tokenize_header_strip(self, text):
        r = re.compile("\r*\n\r*\n")
        new_text = r.split(text)
        header = new_text[0]
        hlines = header.split("\r\n")
        h_to_b = []
        hinfo = {}
        hinfo['domains'] = []
        hinfo['subjects'] = []
        hinfo['ips'] = []
        hinfo['from'] = []
        hinfo['content'] = []
        ip = re.compile('Received:.*\[(\d{,3}.\d{,3}.\d{,3}.\d{,3})\]')
        from_email = re.compile('From:.* [<]?(.*@([^>]*))')
        subject = re.compile('Subject: (.*)')
        content_type = re.compile('Content-Type: ([^;]*)')
        for line in hlines:
            m= ip.match(line)
            if m is None:
                m = from_email.match(line)
                if m is None:
                    m = subject.match(line)
                    if m is not None:
                        hinfo['subjects'] += [m.group(1)]
                    else:
                        m = content_type.match(line)
                        if m is not None:
                            hinfo['content'] += [m.group(1)]
                else:
                    hinfo['from'] += [m.group(1)]
                    hinfo['domains'] += [m.group(2)]
            else:
                hinfo['ips'] += [m.group(1)]
        h_keep = []
        for a in hinfo.values():
            h_keep.extend(a)
        hbody = "\r\n".join(h_keep)
        body = "\r\n".join(new_text[1:])
        body += hbody  # keep retained header info in the body
        (tokens, num_money_tokens) = self.tokenize_money(body)
        return [tokens, hinfo, num_money_tokens]

    def tokenize_everything(self, text):
        tokens = self.tokenize_header_strip(text)
        # right now returning header striped while retaining header info in the body, web, overpunc
        return tokens
